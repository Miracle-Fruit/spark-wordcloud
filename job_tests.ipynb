{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparkjobs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (869658583.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [189]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install pyspark\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp38-cp38-manylinux1_x86_64.whl (371 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.0/372.0 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.6.1\n",
      "  Downloading numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading Pillow-9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.9/930.9 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /home/lukas/Git/BDEA/.venv/lib/python3.8/site-packages (from matplotlib->Wordcloud) (3.0.8)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lukas/Git/BDEA/.venv/lib/python3.8/site-packages (from matplotlib->Wordcloud) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/lukas/Git/BDEA/.venv/lib/python3.8/site-packages (from matplotlib->Wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/lukas/Git/BDEA/.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->Wordcloud) (1.16.0)\n",
      "Installing collected packages: pillow, numpy, kiwisolver, fonttools, cycler, matplotlib, Wordcloud\n",
      "Successfully installed Wordcloud-1.8.1 cycler-0.11.0 fonttools-4.33.3 kiwisolver-1.4.2 matplotlib-3.5.1 numpy-1.22.3 pillow-9.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /home/lukas/Git/BDEA/.venv/lib/python3.8/site-packages (1.4.36)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/lukas/Git/BDEA/.venv/lib/python3.8/site-packages (from sqlalchemy) (1.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in /home/lukas/Git/BDEA/.venv/lib/python3.8/site-packages (1.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark as fs\n",
    "fs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import configparser\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, exc\n",
    "engine = create_engine(\"mysql+pymysql://root:admin@mariadb:8081/data_frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Seassion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': 'admin', 'password': 'admin', 'driver': 'com.mysql.cj.jdbc.Driver'}\n"
     ]
    }
   ],
   "source": [
    "#Create the Database propertiesdb_properties={}\n",
    "db_properties={}\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"database_config.ini\")\n",
    "db_prop = config['mariadb']\n",
    "db_url = db_prop['url']\n",
    "db_properties['user']=db_prop['username']\n",
    "db_properties['password']=db_prop['password']\n",
    "db_properties['driver']=db_prop['driver']\n",
    "print(db_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparkjob 1 TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/01 01:01:37 WARN Utils: Your hostname, be-quiet resolves to a loopback address: 127.0.1.1; using 192.168.178.52 instead (on interface wlp5s0)\n",
      "22/05/01 01:01:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/lukas/.ivy2/cache\n",
      "The jars for the packages stored in: /home/lukas/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-97ddc4eb-a231-416d-8195-bc1a71b4e3e2;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/lukas/Git/BDEA/.venv/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound mysql#mysql-connector-java;8.0.29 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.19.4 in central\n",
      ":: resolution report :: resolve 69ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.19.4 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.29 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-97ddc4eb-a231-416d-8195-bc1a71b4e3e2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n",
      "22/05/01 01:01:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/home/lukas/Git/BDEA/.venv/lib/python3.8/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession,SQLContext,DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                        .master(\"local\")\\\n",
    "                        .appName('TF_job')\\\n",
    "                        .config('spark.jars.packages','mysql:mysql-connector-java:8.0.29')\\\n",
    "                        .config('spark.driver.extraClassPath','mysql-connector-java-8.0.29.jar')\\\n",
    "                        .config('spark.executor.extraClassPath','mysql-connector-java-8.0.29.jar')\\\n",
    "                        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sql_c = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcluate tf for new docs\n",
    "def word_count(txt_file):\n",
    "    table_name = txt_file.split('.')[0] # genarte table name here\n",
    "    txt = sc.textFile(txt_file)\n",
    "    counts = txt.flatMap(lambda line: re.split(\"\\W+\", line.lower())) \\\n",
    "                .filter(lambda word: len(word)>3)\\\n",
    "                .map(lambda word: (word, 1)) \\\n",
    "                .reduceByKey(lambda a, b: a + b)\n",
    "    tf_data = spark.createDataFrame(counts,[\"word\",\"counts\"])\n",
    "    tf_data = tf_data.withColumn(\"name\",lit(str(table_name)))\n",
    "    tf_data.createOrReplaceTempView(\"tf_local\")\n",
    "\n",
    "    word_df = sql_c.sql(\"select word from tf_local\").withColumn(\"df\",lit(1))\n",
    "    word_df.createOrReplaceTempView(\"word_list\")\n",
    "\n",
    "    df_data = spark.read.jdbc(url=db_url,table='df',properties=db_properties)\n",
    "    df_data.createOrReplaceTempView(\"df\")\n",
    "    df_data = sql_c.sql(\"select word, sum(df) as df from (select * from word_list UNION ALL select * from df) z group by word\").sort(\"word\")\n",
    "\n",
    "    df_data.write.jdbc(url=db_url,table='df',mode='overwrite', properties=db_properties)\n",
    "    tf_data.write.jdbc(url=db_url,table='tf',mode='append',properties=db_properties)\n",
    "    # select tf.name,tf.word, tf.counts / df.df from tf join df on df.word = tf.word;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count(\"example2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update DF from all text files\n",
    "def update_df():\n",
    "    # select word, count(word) from tf group by word;\n",
    "    df_tf = spark.read.jdbc(url=db_url,table='tf',properties=db_properties)\n",
    "    df_tf.createOrReplaceTempView(\"tf\")\n",
    "\n",
    "    df_df = sql_c.sql(\"select word, count(word) as df from tf group by word\")\n",
    "    df_df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "    tfidf = sql_c.sql(\"select tf.name,tf.word, tf.counts / df.df as tfidf from tf join df on df.word = tf.word\")\n",
    " \n",
    "    df_df.write.jdbc(url=db_url,table='df',mode='overwrite',properties=db_properties)\n",
    "    tfidf.write.jdbc(url=db_url,table='tfidf',mode='overwrite',properties=db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\"hallo\":67,\"cioau\":100,...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def create_wc_tfidf(file_name):\n",
    "    file_tifidf = sql_c.sql('SELECT word,tfidf FROM tfidf WHERE name = \"{}\"'.format(file_name)).collect()\n",
    "    word_list = {x.word:x.tfidf for x in file_tifidf}\n",
    "    \n",
    "    wordcloud = WordCloud(width = 1000, height = 500,background_color='white')\n",
    "    wordcloud.generate_from_frequencies(word_list)\n",
    "    wordcloud.to_file(\"./wordclouds/\" + file_name + \".png\")\n",
    "\n",
    "\n",
    "def create_wc_all_tfidf():\n",
    "    tfidf = spark.read.jdbc(url=db_url,table='tfidf',properties=db_properties)\n",
    "    tfidf.createOrReplaceTempView(\"tfidf\")\n",
    "    names = tfidf.select(\"name\").distinct().collect()\n",
    "    for name in names:\n",
    "        create_wc_tfidf(name.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
